= Road to Multi-Arch

== Motivation
// Talk about why we want multi-arch, what was the initiator
With the release of Apples M1 chip, ARM became accessable for a brought audience. AWS published to bring spin up some clusters with ARM at their heart ( link:https://aws.amazon.com/ec2/graviton/[AWS Graviton3] ). They prupose a 40% better price performance compared to the current X86-based instances. We therefore assume an siginificant audience being attracted to the better price and higher performance ARM-based systems have to offer. 

As a Start-Up we want to gain traction for our products and try to cover as many developing systems on the market as possible. Although ARM-based systems are announced a while ago, we decided now to go for multi-arch since developer showed interest in Apples M1 based systems. In order to provide the freedom of choice to our team, we decided to use this interest as a kick-off for enabling multi-arch images for our products and operators. 

Please notice that this article is written by somebody who chose a arm64-based system.

// shadow-utils= Definitions
// // define what you are going to talk about, difference between operator- and product-images
// In the following, we will talk about different problems we were facing during developing mulit architecture images. To avoid confusion we'd like to define things proper. If you feel already informed and confident about de difference between Products and Operators as well as the definition of mulit-arch you may skip this section. 

// === Product Image

// Product Images are products which actually work for our customer. We understand things like Kafka, Nifi, Airflow and Superset as our products and those are as (docker-) images in our repository. Important to notice is, that products do not require to compile a binary on build. We just bundle products and prerequisites up in a Image. This image on the other hand requires to be provided in the architecture the customer wants to use. For those, we use docker buildx. 

// === Operators

// Operators represent our infrastructure as code. Those are written in Rust and require compilation for the target architecture and therefore we use cargos target flag to specify the architecture to build for. However, operators are going to be shipped in a image.

// === Multi-Arch-Images

// Multi-Arch-Images are images which are looking from the outside like a usual image. Under the hood, you find a manifest list in your repository which is pointing to different builds for the architectures defined. When pulling a image, docker can decide which architecture it's running on and pulling the correct image accordingly.

== Outline and seperation

What do we do with docker, why cross-compiling with cargo?
Cross-platfrom-builds require some prerequisite and requirements in terms of code, libraries and developement environment. For clarity we would like to make some seperations between techniques necessary to archive multi-platform coverage. 

=== Docker

Docker is the go-to tool for building images. Those images usally have a base image (e.g. nginx, ubi8-minimal, ...) and some stuff installed on. Normally you go for something like:
[source, bash]
----
docker build -f <Dockerfile> -t <tag-of-your-choice> 
----
If you push those images to the repository, a manifest will be created storing all metadata such as digests, architecture, lables and so on. If images got pulled by Docker, it will check the manifest and pull the image if metadata provided matches metadata given (such as tag and version).
For multi-arch, Docker comes with some features to enable the build and the storage in your repository. The following will outline the most important ones.

==== qemu

link:https://docs.docker.com/[Docker] comes with an emulator called qemu, which comes with arichtecture simulations and provides a architecture independed foundation to run software of different platforms within. Furthermore Docker provides a method called link:https://docs.docker.com/build/buildx/multiple-builders/[buildx] which can be used for multi-arch docker-images. How to leverage buildx will be shown in section <<buildx>>. 

==== Manifest Lists

Manifest lists are basically the meta version of a manifest. As the name suggests, manifest list are a list of manifests with different architectures. Each manifest list has a digest, and stores the digests of the corresponding images of different architectures. Those lists can be created in several ways. In our case we directly exported a manifest list to our repository, since we simultaniously build both architectures via a builder instance. You may also build each architecture seperatly on a native machine, and leverage manifest link:https://docs.docker.com/engine/reference/commandline/manifest_create/[create]

[source, bash]
----
docker manifest create MANIFEST_LIST --amend MANIFEST<Platform1> --amend MANIFEST<Platform2>
----

This little larger command will enable you to build all platforms locally, test them and bundel them up to push them into your repository. There is the downside, that the images bundeld up in the new list have to be pushed into the same repository within the same organisation. E.g. stackable/opartarA-arm, stackable/opartorA-amd bundeld into stackable/operatorA. 

=== cargo cross-compiling

The Rust-compiler cargo comes by default with the option to define a target platform using 

[source, bash]
----
cargo build --target=ARCH
----

Thus you can compile x86 executable binaries from a arm64-based system. Important to notice, with cargo you *can not* produce multi-arch binaries. 
In the full context of qemu and multi-arch builds, you might ask the question why to use cargo cross-compiling since the architecture can get emulated. Sadly we do not have a apropriate answer to this question, we only can tell that it is not working without it.

==== Rustup (Toolchain)

If you want to build for a certain architecture, it's necessarry to add the archtecture specific toolchain. In terms of multi-arch, you have to rustup again for the architecture you are already in (qemu emulated architecture). We believe at this point, that this is because qemu is not coming with all the regular libraries provided by a native OS or hase some infrastructre problems.

=== Why combine cargo and docker

As outlined before, docker is required to build images. Since operators are shipped as those, we find a interplay between cargo and docker. On image-build-time, we compile our operator within a docker container. As base image, we use ubi8-minimal hosted by RedHat. As another layer we have a builder (also an image) which executes everything necessary to compile the operator. This leads to the rather complicated interplay between an emulated base image via qemu within a docker build where cargo is building a binary for a specific target.

Please note, that cargos cross-compiling is necessary if and only if you want to compile code within dockers qemu.

=== Dependencies

Since Images are build in layers, you might have at least one dependency for your custom build image such as a base OS. Note that every dependency in the sense of an Image, have to be available in the architecture you target. If you tell Docker to build for different platforms, it will try to retrieve the image in the architecture specified. If it fails to do so, it will default back to any available architecture. This way, it will build mixed images and will not make it obvious to you. There are certain methods to reveal the architecture of your image, we will come to that later.

== Gathered knowledge (Steps to success without further explanation)

// Problems and technical difficulties. Requirements on qemu and cross-compilation
This section is dedicated to some technical requirments and difficulties we found. We will outline the problems and limitations in a wider fashion. 

=== Technical Know-How

We would like to give you a hand with the commands and methods we used with docker and cargo to build and publish (you have to publish) multi-arch images of our products and operators. This section is dedicated to the technical implementation of multi architecture images.  

[#buildx]
==== docker buildx

Docker comes with buildx, which is intended to give a foundation for multi-arch images. This is a extentions to the well known build command. Under the hood, buildx uses a build-kit which is empowered by qemu to produce non-native images on a single architecture. There are certain flags and options you can go with, those will be shown in the following. 
[source, bash]
----
docker buildx build -f <Dockerfile> --platform <platform1>,<platform2> --push
----
This is the basic command to build a image with two different architectures. In principle, there is no further limitation on the platform1 or 2. It is not necessary that one of those platform is a native one. You might have stumbled now over --push. This flag is going to publish your manifest list to repository. This is, becasue docker is not supporting manifest lists in your local repository due to exporting from cache for the builder instance moby. If you compile exclusively for one non-native architecture, you can use --load to load the image from cache to the local docker repository.
As a prerequisite, you need for each instance of non-native architecture a builder. Those builder run on the link:https://hub.docker.com/r/moby/buildkit[moby build], this buildkit is basically providing each architecture you might want to target. Native builds (your current system) do not require emulation and therefore do not need a builder. 

To evoke a builder, you can go with:

[source, bash]
----
docker create --name <builder-name> --use
----

This will produce a instance of moby and tell docker to this as additional builder in parallel. If you want to have more than one you need to give an --append <builder-name> flag (to do so, the instance must exist). With this you can create a abitrary number of instances. 

==== cargo build --target

Cargo comes with an --target option. Basically you can give cargo a target platform to compile the binary for. To have thought about everything you have to do the following steps:
[source, bash]
----
rustup target add <Target architecture> # for arm64 e.g. aarch64-unknown-linux-gnu
----

Having now your toolchain ready you have to set cc, cxx and linker in your environment variables. We will give it as example for arm64:

[source, bash]
----
CARGO_TARGET_AARCH64_UNKNOWN_LINUX_GNU_LINKER=aarch64-linux-gnu-gcc 
CC_aarch64_unknown_linux_gnu=aarch64-linux-gnu-gcc 
CXX_aarch64_unknown_linux_gnu=aarch64-linux-gnu-g++
----

Surely those compiler and linker have to be present on your machine and now you are ready to cross-compile for arm64 on a non-native machine. After reading this, you may ask why we have to go the route with cargo --target and the specifications of linker and c-compiler, since the baseOS will be pulled with the architecture of your machine or the one from your emulation. Exactly that's the point, qemu got a flaw when it comes to compiling with certain libraries such as link:https://crates.io/crates/unicode-bidi[unicode-bidi]. If we compile within qemu we will end up with an segmentation fault, which is a current and known link:https://github.com/rust-lang/rust/issues/94967[issue]. 

=== Testing and checking multi-architecture images

This section is dedicated to how to test the architecture of your image. There are surely a few ways how to get do this, here we will outline the most decent ones in our opinion. 

==== Docker inspect

Once your Manifest List in your repositroy, you might wanna check if all your architectures where build. To do so you can leverage dockers inspect command:

[source, bash]
----
docker manifest inspect <Tag>
----
This will output the manifest list with all the architectures referenced within. This will only check, which architectures where build compared to what was supposed to do. If you build in parallel, usually if one architecture fails to build, the whole building process is supposed to fail.

A better overview will be given when you pull all specific architectures from your repositiory one by one. To do so you can do 

[source, bash]
----
docker pull <Tag> --platform <Platform>
----

From there you can go and 

[source, bash]
----
docker image inspect <IMAGE-ID>
----

This will output some metadata. The interesting part are the fields 'Architecture' and 'architecture'. If both are showing the supposed tags, you should have a pure image. Before you keep progressing from this point and pull the next architecture of your image, don't forget to delete the old one to avoid overlaps or problems.

==== Binutils (objdump)

Another nice way to make sure that things are working as expected, is to pull the image you want to check, run it with:

[source, bash]
----
docker run -it --entrypoint bash --user root <Image-Tag>
----

You may need root access here since we want to install binutils via apt-get, microdnf or yum depending on what is available on your image. If you have installed the package you can run:

[source, bash]
----
objdump -f /usr/lib/libssl.so
objdump -f path-to-comnpiled-binary
----

objdump will give you informations about the architecture of the OS and the comnpiled binary as well. This way you can ensure that binary and operating system have the architecture desired.


// === Base Images

// Base images are basically the OS we use for the container. We use ubi8-minimal which is available as multi-arch in the RedHat repository. We didn't have to take any former actions here. Since the latest update from RedHat, we are forced to pin that version. We enabled renovate to check newest versions. 

// === Product Images

// In order to have Product Images in multi-arch we only had to change from 
// [source, bash]
// ----
// docker build -t <tag> -f <dockerfile>
// docker push -t <tag>
// ----
// to
// [source, bash]
// ----
// docker buildx build -t <tag> -f <dockerfile> --platform <platform1><platform2> --push
// ----
// This made it possible to have multi arch images in Nexus. This is, because we do not have to compile anything to build product images. Under the hood, docker is building simultanious both images for platform 1 and for platform 2. After the build is complete, we will have a link:https://docs.docker.com/engine/reference/commandline/manifest/[manifest] list containing all images build during that process. 

// === Operators

// For operators this is a different story. We encountered several problems with docker respective with qemu due to certain flaws of the emulation. For making this work, we needed to tell cargo precisly what we compile for. We had to follow the whole process to multi-arch as if we would compile for a non-native architecture. Although we basically are in the target architecture (since emulated) we needed to have the toolchain downloaded and linker and c-compiler setted in the environment. Because of a library called unicode-bidi, we encountered segmentation faults during compile time. This was not possible to solve without giving cargo a specific target as if the base os was non-native. 

== Limitations

Folowing limitations have been encountered:
* OpenSSL:: 
    We fixed a compile error with OpenSSL by vendoring it link:https://stackoverflow.com/questions/65553557/why-rust-is-failing-to-build-command-for-openssl-sys-v0-9-60-even-after-local-in[issue on stackoverflow]
* Local manifest lists::
    Currently it is not possible to export manifest lists on a local repository. This is a limitation due to docker.

== Outlook
// Talk about GH-Actions and what we want to achieve in the future