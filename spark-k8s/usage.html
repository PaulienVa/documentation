<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Usage :: Stackable Documentation</title>
    <link rel="canonical" href="https://docs.stackable.tech/spark-k8s/usage.html">
    <meta name="generator" content="Antora 3.0.1">
    <link rel="stylesheet" href="../_/css/site.css">
    <script>var uiRootPath = '../_'</script>
    <link rel="icon" href="../_/img/favicon.png" type="image/png">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="container">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://stackable.de"><img src="../_/img/stackable-logo.png"></a>
      <a class="navbar-item documentation-link" href="https://docs.stackable.tech">Documentation</a>
      <div class="navbar-item search hide-for-print">
        <div id="search-field" class="field">
          <input id="search-input" type="text" placeholder="Search the docs">
        </div>
      </div>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <a href="https://www.stackable.de/en/contact/" class="button pull-right">Contact Us</a>
    </div>
      <div id="topbar-nav" class="navbar-menu">
      </div>
      </div>
    </nav>
    <nav class="navbar-sub">
      <div class="container">
        <a class="navbar-sub-item" href="../home/index.html">Home</a>
<a class="navbar-sub-item" href="../home/getting_started.html">Getting Started</a>
<a class="navbar-sub-item" href="../home/tutorials/end-to-end_data_pipeline_example.html">Tutorials</a>
<div class="navbar-sub-item drop-down">
    Operators
    <div class="drop-down-content">
    <a class="drop-down-item" href="../home/operators/index.html">Overview</a>
    <a class="drop-down-item" href="../airflow/index.html">Apache Airflow</a>
    <a class="drop-down-item" href="../druid/index.html">Apache Druid</a>
    <a class="drop-down-item" href="../hbase/index.html">Apache HBase</a>
    <a class="drop-down-item" href="../hdfs/index.html">Apache Hadoop HDFS</a>
    <a class="drop-down-item" href="../hive/index.html">Apache Hive</a>
    <a class="drop-down-item" href="../kafka/index.html">Apache Kafka</a>
    <a class="drop-down-item" href="../nifi/index.html">Apache NiFi</a>
    <a class="drop-down-item" href="../spark/index.html">Apache Spark (standalone)</a>
    <a class="drop-down-item" href="index.html">Apache Spark on K8S</a>
    <a class="drop-down-item" href="../superset/index.html">Apache Superset</a>
    <a class="drop-down-item" href="../trino/index.html">Trino</a>
    <a class="drop-down-item" href="../zookeeper/index.html">Apache ZooKeeper</a>
    <a class="drop-down-item" href="../opa/index.html">OpenPolicyAgent</a>
    <a class="drop-down-item" href="../commons-operator/index.html">Commons</a>
    <a class="drop-down-item" href="../secret-operator/index.html">Secret</a>
    </div>
</div>
<a class="navbar-sub-item" href="../home/contributor/index.html">Contribute</a>

        <a class="arrow" href="javascript:document.querySelector('.navbar-sub').classList.toggle('open')">
          <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="arrow-right" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"></path></svg>
        </a>
      </div>
    </nav>
  </header>
<div class="body">
    <div class="container">
<div class="nav-container" data-component="spark-k8s" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html">Stackable Operator for Apache Spark on Kubernetes</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="installation.html">Installation</a>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <a class="nav-link" href="usage.html">Usage</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="job_dependencies.html">Job Dependencies</a>
  </li>
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="rbac.html">RBAC</a>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Stackable Operator for Apache Spark on Kubernetes</span>
    <span class="version">master</span>
  </div>
  <ul class="components">
    <li class="component">
      <a class="title" href="../commons-operator/index.html">Stackable Commons Operator</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../commons-operator/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../home/index.html">Stackable Documentation</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../home/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../airflow/index.html">Stackable Operator for Apache Airflow</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../airflow/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../druid/index.html">Stackable Operator for Apache Druid</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../druid/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../hbase/index.html">Stackable Operator for Apache HBase</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../hbase/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../hdfs/index.html">Stackable Operator for Apache HDFS</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../hdfs/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../hive/index.html">Stackable Operator for Apache Hive</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../hive/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../kafka/index.html">Stackable Operator for Apache Kafka</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../kafka/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../nifi/index.html">Stackable Operator for Apache NiFi</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../nifi/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../spark/index.html">Stackable Operator for Apache Spark</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../spark/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component is-current">
      <a class="title" href="index.html">Stackable Operator for Apache Spark on Kubernetes</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../superset/index.html">Stackable Operator for Apache Superset</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../superset/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../zookeeper/index.html">Stackable Operator for Apache ZooKeeper</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../zookeeper/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../opa/index.html">Stackable Operator for OPA</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../opa/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../trino/index.html">Stackable Operator for Trino</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../trino/index.html">master</a>
        </li>
      </ul>
    </li>
    <li class="component">
      <a class="title" href="../secret-operator/index.html">Stackable Secret Operator</a>
      <ul class="versions">
        <li class="version is-latest">
          <a href="../secret-operator/index.html">master</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../home/index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">Stackable Operator for Apache Spark on Kubernetes</a></li>
    <li><a href="usage.html">Usage</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/stackabletech/spark-k8s-operator/edit/main/docs/modules/ROOT/pages/usage.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Usage</h1>
<div class="sect1">
<h2 id="_create_an_apache_spark_job"><a class="anchor" href="#_create_an_apache_spark_job"></a>Create an Apache Spark job</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you followed the installation instructions, you should now have a Stackable Operator for Apache Spark up and running and you are ready to create your first Apache Spark kubernetes cluster.</p>
</div>
<div class="paragraph">
<p>The example below creates a job running on Apache Spark 3.2.1, using the spark-on-kubernetes paradigm described in the spark documentation. The application file is itself part of the spark distribution and <code>local</code> refers to the path on the driver/executors; there are no external dependencies.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>cat &lt;&lt;EOF | kubectl apply -f -
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: spark-clustermode-001
spec:
  version: 3.2.1-hadoop3.2
  mode: cluster
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: local:///stackable/spark/examples/jars/spark-examples_2.12-3.2.1.jar
  image: 3.2.1-hadoop3.2
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
  executor:
    cores: 1
    instances: 3
    memory: "512m"
EOF</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_examples"><a class="anchor" href="#_examples"></a>Examples</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following examples have the following <code>spec</code> fields in common:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>version</code>: the current version is "1.0"</p>
</li>
<li>
<p><code>sparkImage</code>: the docker image that will be used by job, driver and executor pods. This can be provided by the user.</p>
</li>
<li>
<p><code>mode</code>: only <code>cluster</code> is currently supported</p>
</li>
<li>
<p><code>mainApplicationFile</code>: the artifact (Java, Scala or Python) that forms the basis of the Spark job.</p>
</li>
<li>
<p><code>args</code>: these are the arguments passed directly to the application. In the examples below it is e.g. the input path for part of the public New York taxi dataset.</p>
</li>
<li>
<p><code>sparkConf</code>: these list spark configuration settings that are passed directly to <code>spark-submit</code> and which are best defined explicitly by the user. Since the <code>SparkApplication</code> "knows" that there is an external dependency (the s3 bucket where the data and/or the application is located) and how that dependency should be treated (i.e. what type of credential checks are required, if any), it is better to have these things declared together.</p>
</li>
<li>
<p><code>volumes</code>: refers to any volumes needed by the <code>SparkApplication</code>, in this case an underlying <code>PersistentVoulmeClaim</code>.</p>
</li>
<li>
<p><code>driver</code>: driver-specific settings, including any volume mounts.</p>
</li>
<li>
<p><code>executor</code>: executor-specific settings, including any volume mounts.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Job-specific settings are annotated below.</p>
</div>
<div class="sect2">
<h3 id="_pyspark_externally_located_artifact_and_dataset"><a class="anchor" href="#_pyspark_externally_located_artifact_and_dataset"></a>Pyspark: externally located artifact and dataset</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-external-dependencies
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.2.1-hadoop3.2-python39-stackable0.1.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny_tlc_report.py <i class="conum" data-value="1"></i><b>(1)</b>
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'" <i class="conum" data-value="2"></i><b>(2)</b>
  deps:
    requirements:
      - tabulate==0.8.9  <i class="conum" data-value="3"></i><b>(3)</b>
  sparkConf:  <i class="conum" data-value="4"></i><b>(4)</b>
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
    "spark.executor.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
  volumes:
    - name: job-deps  <i class="conum" data-value="5"></i><b>(5)</b>
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies  <i class="conum" data-value="6"></i><b>(6)</b>
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies  <i class="conum" data-value="6"></i><b>(6)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Job python artifact (external)</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Job argument (external)</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>List of python job requirements: these will be installed in the pods via <code>pip</code></td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Spark dependencies: the credentials provider (the user knows what is relevant here) plus dependencies needed to access external resources (in this case, in s3)</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>the name of the volume mount backed by a <code>PersistentVolumeClaim</code> that must be pre-existing</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>the path on the volume mount: this is referenced in the <code>sparkConf</code> section where the extra class path is defined for the driver and executors</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_pyspark_externally_located_dataset_artifact_available_via_pvcvolume_mount"><a class="anchor" href="#_pyspark_externally_located_dataset_artifact_available_via_pvcvolume_mount"></a>Pyspark: externally located dataset, artifact available via PVC/volume mount</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-image
  namespace: default
spec:
  version: "1.0"
  image: docker.stackable.tech/stackable/ny-tlc-report:0.1.0 <i class="conum" data-value="1"></i><b>(1)</b>
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.2.1-hadoop3.2-python39-stackable0.1.0
  mode: cluster
  mainApplicationFile: local:///stackable/spark/jobs/ny_tlc_report.py <i class="conum" data-value="2"></i><b>(2)</b>
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'" <i class="conum" data-value="3"></i><b>(3)</b>
  deps:
    requirements:
      - tabulate==0.8.9 <i class="conum" data-value="4"></i><b>(4)</b>
  sparkConf: <i class="conum" data-value="5"></i><b>(5)</b>
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
    "spark.executor.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
  volumes:
    - name: job-deps <i class="conum" data-value="6"></i><b>(6)</b>
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies <i class="conum" data-value="7"></i><b>(7)</b>
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies <i class="conum" data-value="7"></i><b>(7)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Job image: this contains the job artifact that will retrieved from the volume mount backed by the PVC</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Job python artifact (local)</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Job argument (external)</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>List of python job requirements: these will be installed in the pods via <code>pip</code></td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Spark dependencies: the credentials provider (the user knows what is relevant here) plus dependencies needed to access external resources (in this case, in s3)</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>the name of the volume mount backed by a <code>PersistentVolumeClaim</code> that must be pre-existing</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>the path on the volume mount: this is referenced in the <code>sparkConf</code> section where the extra class path is defined for the driver and executors</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_jvm_scala_externally_located_artifact_and_dataset"><a class="anchor" href="#_jvm_scala_externally_located_artifact_and_dataset"></a>JVM (Scala): externally located artifact and dataset</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-pvc
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.2.1-hadoop3.2-stackable0.4.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.0-SNAPSHOT.jar <i class="conum" data-value="1"></i><b>(1)</b>
  mainClass: org.example.App <i class="conum" data-value="2"></i><b>(2)</b>
  args:
    - "'s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  sparkConf: <i class="conum" data-value="3"></i><b>(3)</b>
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
    "spark.executor.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
  volumes:
    - name: job-deps <i class="conum" data-value="4"></i><b>(4)</b>
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies <i class="conum" data-value="5"></i><b>(5)</b>
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies <i class="conum" data-value="5"></i><b>(5)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Job artifact located on S3.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Job main class</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Spark dependencies: the credentials provider (the user knows what is relevant here) plus dependencies needed to access external resources (in this case, in s3, accessed without credentials)</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>the name of the volume mount backed by a <code>PersistentVolumeClaim</code> that must be pre-existing</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>the path on the volume mount: this is referenced in the <code>sparkConf</code> section where the extra class path is defined for the driver and executors</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_jvm_scala_externally_located_artifact_accessed_with_credentials"><a class="anchor" href="#_jvm_scala_externally_located_artifact_accessed_with_credentials"></a>JVM (Scala): externally located artifact accessed with credentials</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-s3-private
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.2.1-hadoop3.2-stackable0.4.0
  mode: cluster
  mainApplicationFile: s3a://my-bucket/spark-examples_2.12-3.2.1.jar <i class="conum" data-value="1"></i><b>(1)</b>
  mainClass: org.apache.spark.examples.SparkPi <i class="conum" data-value="2"></i><b>(2)</b>
  s3: <i class="conum" data-value="3"></i><b>(3)</b>
    credentialsSecret: minio-credentials  <i class="conum" data-value="4"></i><b>(4)</b>
    endpoint: http://test-minio:9000/
  sparkConf: <i class="conum" data-value="5"></i><b>(5)</b>
    spark.hadoop.fs.s3a.aws.credentials.provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider" <i class="conum" data-value="6"></i><b>(6)</b>
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.driver.extraClassPath: "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
    spark.executor.extraClassPath: "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
  volumes:
    - name: spark-pi-deps <i class="conum" data-value="7"></i><b>(7)</b>
      persistentVolumeClaim:
        claimName: spark-pi-pvc
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: spark-pi-deps
        mountPath: /dependencies <i class="conum" data-value="8"></i><b>(8)</b>
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    volumeMounts:
      - name: spark-pi-deps
        mountPath: /dependencies <i class="conum" data-value="8"></i><b>(8)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Job python artifact (located in S3)</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Artifact class</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>S3 section, specifying the existing secret and S3 end-point (in this case, MinIO)</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Credentials secret</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Spark dependencies: the credentials provider (the user knows what is relevant here) plus dependencies needed to access external resources&#8230;&#8203;</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>&#8230;&#8203;in this case, in s3, accessed with the credentials defined in the secret</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>the name of the volume mount backed by a <code>PersistentVolumeClaim</code> that must be pre-existing</td>
</tr>
<tr>
<td><i class="conum" data-value="8"></i><b>8</b></td>
<td>the path on the volume mount: this is referenced in the <code>sparkConf</code> section where the extra class path is defined for the driver and executors</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_jvm_scala_externally_located_artifact_accessed_with_job_arguments_provided_via_configuration_map"><a class="anchor" href="#_jvm_scala_externally_located_artifact_accessed_with_job_arguments_provided_via_configuration_map"></a>JVM (Scala): externally located artifact accessed with job arguments provided via configuration map</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: v1
kind: ConfigMap
metadata:
  name: cm-job-arguments <i class="conum" data-value="1"></i><b>(1)</b>
data:
  job-args.txt: |
    s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: ny-tlc-report-configmap
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.2.1-hadoop3.2-stackable0.4.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.1.0.jar <i class="conum" data-value="3"></i><b>(3)</b>
  mainClass: tech.stackable.demo.spark.NYTLCReport
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
    - name: cm-job-arguments
      configMap:
        name: cm-job-arguments <i class="conum" data-value="4"></i><b>(4)</b>
  args:
    - "--input /arguments/job-args.txt" <i class="conum" data-value="5"></i><b>(5)</b>
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
    "spark.executor.extraClassPath": "/dependencies/jars/hadoop-aws-3.2.0.jar:/dependencies/jars/aws-java-sdk-bundle-1.11.375.jar"
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
      - name: cm-job-arguments <i class="conum" data-value="6"></i><b>(6)</b>
        mountPath: /arguments  <i class="conum" data-value="7"></i><b>(7)</b>
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
      - name: cm-job-arguments <i class="conum" data-value="6"></i><b>(6)</b>
        mountPath: /arguments <i class="conum" data-value="7"></i><b>(7)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Name of the configuration map</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Argument required by the job</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Job scala artifact that requires an input argument</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The volume backed by the configuration map</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>The expected job argument, accessed via the mounted configuration map file</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>The name of the volume backed by the configuration map that will be mounted to the driver/executor</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>The mount location of the volume (this will contain a file <code>/arguments/job-args.txt</code>)</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_crd_argument_coverage"><a class="anchor" href="#_crd_argument_coverage"></a>CRD argument coverage</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Below are listed the CRD fields that can be defined by the user:</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">CRD field</th>
<th class="tableblock halign-left valign-top">Remarks</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>apiVersion</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spark.stackable.tech/v1alpha1</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>kind</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>SparkApplication</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>metadata.name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Job name</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.version</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">"1.0"</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.mode</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>cluster</code> or <code>client</code>. Currently only <code>cluster</code> is supported</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.image</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">User-supplied image containing spark-job dependencies that will be copied to the specified volume mount</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.sparkImage</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Spark image which will be deployed to driver and executor pods, which must contain spark environment needed by the job e.g. <code>docker.stackable.tech/stackable/spark-k8s:3.2.1-hadoop3.2-stackable0.4.0</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.mainApplicationFile</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The actual application file that will be called by <code>spark-submit</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.mainClass</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The main class i.e. entry point for JVM artifacts</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.args</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Arguments passed directly to the job artifact</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.s3.credentialsSecret</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of the credentials secret for S3 access</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.s3.endpoint</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">S3 endpoint</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.sparkConf</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A map of key/value strings that will be passed directly to <code>spark-submit</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.deps.requirements</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of python packages that will be installed via <code>pip</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.deps.packages</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of packages that is passed directly to <code>spark-submit</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.deps.excludePackages</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of excluded packages that is passed directly to <code>spark-submit</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.deps.repositories</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of repositories that is passed directly to <code>spark-submit</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.volumes</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of volumes</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.volumes.name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The volume name</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.volumes.persistentVolumeClaim.claimName</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">The persistent volume claim backing the volume</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.cores</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Number of cores used by the driver (only in cluster mode)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.coreLimit</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Total cores for all executors</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.memory</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Specified memory for the driver</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.volumeMounts</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of mounted volumes for the driver</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.volumeMounts.name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of mount</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.driver.volumeMounts.mountPath</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume mount path</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.cores</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Number of cores for each executor</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.instances</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Number of executor instances launched for this job</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.memory</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Memory specified for executor</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.volumeMounts</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">A list of mounted volumes for each executor</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.volumeMounts.name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Name of mount</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>spec.executor.volumeMounts.mountPath</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Volume mount path</p></td>
</tr>
</tbody>
</table>
</div>
</div>
</article>
  </div>
</main>
    </div>
</div>
<footer class="footer">
</footer>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
<script src="../_/js/vendor/lunr.js"></script>
<script src="../_/js/search-ui.js" id="search-ui-script" data-site-root-path=".." data-snippet-length="100" data-stylesheet="../_/css/search.css"></script>
<script async src="../search-index.js"></script>
  </body>
</html>
